Project details

Your projects will be submitted by putting all required files into a
subdirectory of the class github repository.  The directory should be named:

projects/YOURONIDLOGIN

(where YOURONIDLOGIN is your ONID login -- e.g., for me, it would be projects/alex)

Part 1:  PROJECT PROPOSAL
Due:     April 19
File(s): proposal.pdf
Points:  15

Describe your project plan.  What is your idea for test generation?
You should cite and include links to at least one relevant paper in
the literature (though you can propose something highly original, you
are likely to be basing your approach on something existing, in part,
or adopting a method from another field).  Why do you expect this
method to possibly be effective?

Should be about 500 words.  Can be more, probably should not be less.

Where can you get ideas?  Look in the readings.txt file.

Also, look at recent ISSTA and ICST papers.  Also, many ICSE, FSE,
ASE, ISSRE, and even SPLASH/OOPSLA or PLDI papers concern testing.

American Fuzzy Lop is a powerful and popular fuzzer that might give
ideas, if you look it up.  However, it aims at mutating (modifying)
files to test programs, not sequence-based testing.

Part 2:  COMPETITIVE MILESTONE 1
Due:     May 3
File(s): tester1.py doc1.pdf
Points:  20

At this point, you should have a basic implementation of a novel test
generation algorithm using the TSTL API.  This should be implemented
in a file called tester1.py.  tester1.py should import sut.py and test
it.  The arguments should be read from the command line, in order, as
follows:

<timeout>: time in seconds for testing -- your tester must respect
this, within a small margin of error

<seed>: seed for Python Random.random object used for random number
generation in your code, if it is stochastic

<depth>: maximum length of a test generated by your algorithm

<width>: maximum memory/BFS queue/other parameter that is basically a
search width

<faults>: either 0 or 1 depending on whether your tester should check
for faults in the SUT; if true, you should save a test case for each
discovered failure (terminating the test that generated it), in the
current directory, as failure1.test failure2.test, etc.

<coverage>: either 0 or 1 depending on whether a final coverage report
should be produced, using TSTL's internalReport() function.  Your
tester is allowed to assume that coverage is being collected to guide
tests.  

<running>: either 0 or 1 depending on whether running info on branch
coverage should be produced.  The format for output should be the same
as in the --running option of the TSTL randomtester.py implementation:

[elapsed time since start of testing] [total branch count] New branch [actual branch]

see the code in randomtester.py:

                if sut.newBranches() != set([]):
                    print "ACTION:",a[0],tryStutter
                    for b in sut.newBranches():
                        print elapsed,len(sut.allBranches()),"New branch",b
                    sawNew = True

Any other parameters that your tools need should be discovered by
querying the SUT, or by setting sensible defaults.  IF you have
multiple settings you want to experiment with, you can name files
like: tester1.1.py tester1.2.py etc., and we will make use of all of
these.

So, for example, your tester might be called as follows:

python tester1.py 30 1 100 1 0 1 1

This will test sut.py for 30 seconds, with tests of maximum length
100, a very narrow width (1), and not report or check for failures.
It will collect and output an internal coverage report, and also
report each new branch as it is discovered.

At each milestone, we will run all submitted testing algorithms
multiple times, with multiple seeds and parameters (but I am keeping
the settings I'll use secret) on several of the built-in SUTs included
with TSTL, and possibly some others from last term's 562 class.  At
minimum, we will run avlblocks.tstl on the AVL tree, in some buggy
versions.

doc1.pdf should, in at least 500 words, explain your algorithm.

Part 3:  COMPETITIVE MILESTONE 2
Due:     May 18
File(s): tester2.py doc2.pdf
Points:  20

This works the same as last time, but we may vary the parameters and
SUTs.  doc2.pdf is revised documentation for changes to your algorithm.

Part 4:  FINAL SUBMISSION
Due:     June 6 (first day of finals)
File(s): finaltester.py final.pdf mytester.py
Points:  45

The tester submission is the same as last time.  However, this time
you may also submit a more-flexible verison of your algorithm as
mytester.py, which takes more parameters, or has a nicer configuration
interface, like the TSTL randomtester and other generators.  final.pdf
should, in at least 1,000 words, document (include graphs) some effort
to evaluate your algorithm, describe related work, etc.  Basically
final.pdf should be the core of a potential research paper submission
on your novel algorithm.  Of course, your method may be ineffective,
or too small a delta over an existing method to publish, but your aim
is to get as close to publishable results as possible!  If your method
did not fare well (results from the competitive bakeoffs will be
available), explain why.


